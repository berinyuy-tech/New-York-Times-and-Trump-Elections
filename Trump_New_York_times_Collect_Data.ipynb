{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for getting article information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all articles starting in the beginning of June 2015 until now, and we will gather them in two week periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dates = []\n",
    "for month in [\"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]:\n",
    "    for date_pair in [(\"01\", \"15\"), (\"16\", \"31\")]:\n",
    "        date_range = (\"2015\" + month + date_pair[0], \"2015\" + month + date_pair[1])\n",
    "        all_dates.append(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for month in [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"]:\n",
    "    for date_pair in [(\"01\", \"15\"), (\"16\", \"31\")]:\n",
    "        date_range = (\"2016\" + month + date_pair[0], \"2016\" + month + date_pair[1])\n",
    "        all_dates.append(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20150601', '20150615'),\n",
       " ('20150616', '20150631'),\n",
       " ('20150701', '20150715'),\n",
       " ('20150716', '20150731'),\n",
       " ('20150801', '20150815'),\n",
       " ('20150816', '20150831'),\n",
       " ('20150901', '20150915'),\n",
       " ('20150916', '20150931'),\n",
       " ('20151001', '20151015'),\n",
       " ('20151016', '20151031'),\n",
       " ('20151101', '20151115'),\n",
       " ('20151116', '20151131'),\n",
       " ('20151201', '20151215'),\n",
       " ('20151216', '20151231'),\n",
       " ('20160101', '20160115'),\n",
       " ('20160116', '20160131'),\n",
       " ('20160201', '20160215'),\n",
       " ('20160216', '20160231'),\n",
       " ('20160301', '20160315'),\n",
       " ('20160316', '20160331'),\n",
       " ('20160401', '20160415'),\n",
       " ('20160416', '20160431'),\n",
       " ('20160501', '20160515'),\n",
       " ('20160516', '20160531'),\n",
       " ('20160601', '20160615'),\n",
       " ('20160616', '20160631'),\n",
       " ('20160701', '20160715'),\n",
       " ('20160716', '20160731'),\n",
       " ('20160801', '20160815'),\n",
       " ('20160816', '20160831')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(search_term, dates):\n",
    "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "    api_infos = []\n",
    "    # get all urls\n",
    "    for start_date, end_date in dates: \n",
    "        for page in range(101):\n",
    "            queries = {\n",
    "          'api-key': XXX,\n",
    "          'q': search_term,\n",
    "          'begin_date': start_date, \n",
    "          'end_date': end_date, \n",
    "          'page' : page\n",
    "            }\n",
    "            req_t = requests.get(url, params = queries)\n",
    "            adict = json.loads(req_t.text)\n",
    "            doclist = adict['response']['docs']\n",
    "            for j in range(len(doclist)):\n",
    "                api_info = ([doclist[j][\"web_url\"], doclist[j][\"headline\"],\n",
    "                      doclist[j][\"section_name\"],\n",
    "                      doclist[j][\"word_count\"], doclist[j][\"type_of_material\"], \n",
    "                      doclist[j][\"pub_date\"]])\n",
    "                api_infos.append(api_info)\n",
    "    # transform data into dataset\n",
    "    api_dataset = pd.DataFrame(api_infos, columns=['Url','Headline', \n",
    "                                           'Section_name', 'Word_Count', \n",
    "                                           'Type', 'Pub_Date'])\n",
    "    return api_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(url_list):\n",
    "    articles = []\n",
    "    for address in url_list: \n",
    "        try:\n",
    "            article = Article(address)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            article_info = ([article.publish_date, article.authors,\n",
    "                      article.summary, article.keywords, article.text,\n",
    "                     article.url])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        articles.append(article_info)\n",
    "        \n",
    "    newspaper_dataset = pd.DataFrame(articles, columns=['Date','Authors', 'Summary', 'Keywords', 'Text', \"Url\"])\n",
    "    newspaper_dataset[\"Text_Decode\"] = newspaper_dataset[\"Text\"].apply(lambda x: x.encode('utf-8'))\n",
    "    newspaper_dataset[\"Text_Clean\"] = newspaper_dataset[\"Text_Decode\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "    newspaper_dataset = newspaper_dataset.drop('Text_Decode', 1)\n",
    "    return newspaper_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Hillary and Trump Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HC_articles = get_articles(\"Hillary Clinton\", all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hillary_text = get_text(HC_articles[\"Url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trump_articles = get_articles(\"Donald Trump\", all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Total_Trump = get_text(Trump_articles[\"Url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, Merge, and Pickle Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine text and api datasets, get rid of duplicates and bad links, and add a column of Text without the \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trump_merged = Trump_articles.merge(Total_Trump, on = \"Url\", how = \"inner\")\n",
    "good_links = Trump_merged[Trump_merged[\"Text\"] != \"NYTimes.com no longer supports Internet Explorer 9 or earlier. Please upgrade your browser.\"]\n",
    "Trump_no_duplicates = good_links.drop_duplicates(\"Text\")\n",
    "Trump_no_duplicates[\"Text_Decode\"] = Trump_no_duplicates[\"Text\"].apply(lambda x: x.encode('utf-8'))\n",
    "Trump_no_duplicates[\"Text_Clean\"] = Trump_no_duplicates[\"Text_Decode\"].apply(lambda x: x.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hillary_merged = all_newspaper_articles.merge(HC_articles, on = \"Url\", how = \"inner\")\n",
    "Hillary_no_duplicates = Hillary_merged.drop_duplicates(\"Text\")\n",
    "Hillary_no_duplicates = Hillary_no_duplicates[Hillary_no_duplicates[\"Text\"] != \"NYTimes.com no longer supports Internet Explorer 9 or earlier. Please upgrade your browser.\"]\n",
    "final_Hillary = Hillary_no_duplicates\n",
    "final_Hillary[\"Text_Decode\"] = final_Hillary[\"Text\"].apply(lambda x: x.encode('utf-8'))\n",
    "final_Hillary[\"Text_Clean\"] = final_Hillary[\"Text_Decode\"].apply(lambda x: x.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_Hillary[\"Trump_subject\"] = 0\n",
    "Trump_no_duplicates[\"Trump_subject\"] = 1\n",
    "Trump_Clinton_dataset = final_Hillary.append(Trump_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Trump_Clinton_dataset.pkl', 'w') as picklefile:\n",
    "    pickle.dump(Trump_Clinton_dataset, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
